{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ecf179-e212-4886-b54c-f1a0a04bfa8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define os parâmetros da Job\n",
    "dbutils.widgets.text('p_path_file_location','')\n",
    "dbutils.widgets.text('p_layer','')\n",
    "dbutils.widgets.text('p_file_name_raw','')\n",
    "dbutils.widgets.text('p_file_type_raw','')\n",
    "dbutils.widgets.text('p_mode_write_bronze','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499ccde6-937a-4382-9872-43926f01d784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Premissa: Escolher um conjunto de dados do Kaggle relacionado a vendas. \n",
    "Obs.: Certifique-se de que o conjunto de dados inclui informações como datas, produtos, quantidades vendidas, etc.\n",
    "\n",
    "**Dataset escolhido**: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552ee890-8431-4102-a998-7ad47b23f78a",
     "showTitle": true,
     "title": "Importa bilbiotecas"
    }
   },
   "outputs": [],
   "source": [
    "# Importando Bibliotecas\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45969bdd-4520-490c-a9df-c871dc08747b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Funções para leitura, processamento e gravação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643a8874-def1-4a0f-a9fb-47882538848c",
     "showTitle": true,
     "title": "Cria função para leitura de arquivo .csv"
    }
   },
   "outputs": [],
   "source": [
    "def ler_csv(path :str, layer :str, file_name_raw :str, file_type_raw :str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Lê um arquivo CSV do ADSL Gen2.\n",
    "\n",
    "    Args:\n",
    "        layer (str): Camada do arquivo.\n",
    "        file_name (str): Nome do arquivo.\n",
    "        file_type (str): Tipo do arquivo.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: O DataFrame resultante da leitura do arquivo CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Opção de leitura do arquivo CSV\n",
    "        infer_schema = \"true\"\n",
    "        first_row_is_header = \"true\"\n",
    "        delimiter = \",\"\n",
    "\n",
    "        # Lê arquivo csv\n",
    "        df = spark.read.format(file_type_raw)\\\n",
    "            .option('header',first_row_is_header)\\\n",
    "            .option('inferschema',infer_schema)\\\n",
    "            .load(f'{path}/{layer}/{file_name_raw}.{file_type_raw}')\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo CSV: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845a63f4-d168-4a65-8fc2-39923e42e483",
     "showTitle": true,
     "title": "Cria função para inserir coluna row_ingestion_timestamp"
    }
   },
   "outputs": [],
   "source": [
    "def add_timestamp(df:DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adiciona uma coluna com a data e hora atuais a um DataFrame do Spark.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame do Spark.\n",
    "        nome_coluna (str): Nome da coluna a ser adicionada.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: O DataFrame original com a nova coluna adicionada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Adiciona uma coluna com a data e hora atuais\n",
    "        df_timestamp = df.withColumn('row_ingestion_timestamp', current_timestamp())\n",
    "        \n",
    "        return df_timestamp\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao adicionar timestamp: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5cc8cb-62a9-4834-a0ea-7888e487996e",
     "showTitle": true,
     "title": "Função para gravar os dados em parquet no ADSL"
    }
   },
   "outputs": [],
   "source": [
    "def gravar_parquet(df :DataFrame, destination_path :str, mode_write_bronze :str) -> None:\n",
    "    \"\"\"\n",
    "    Grava os dados de um DataFrame no formato Parquet.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame do Spark.\n",
    "        caminho_destino (str): Caminho de destino para salvar o arquivo Parquet.\n",
    "    \"\"\"\n",
    "    # Grava o DataFrame no formato Parquet\n",
    "    df.write.mode(mode_write_bronze).parquet(destination_path)\n",
    "\n",
    "    print(\"Dados salvos com sucesso no formato Parquet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581f7459-72d6-4f96-a65f-75a124506a40",
     "showTitle": true,
     "title": "Função para criar uma delta table a partir de um parquet"
    }
   },
   "outputs": [],
   "source": [
    "def criar_tabela_databricks(file_path_parquet :str, table_name :str) -> None:\n",
    "    \"\"\"\n",
    "    Cria tabela no formato delta no Databricks a partir de um arquivo parquet\n",
    "\n",
    "    Args:\n",
    "        file_path_parquet: Caminho da container (pasta) onde se encontra o arquivo parquet\n",
    "        schema_name: Nome do Schema \n",
    "        table_name: Nome da tabela\n",
    "    \"\"\"\n",
    "\n",
    "    # Verifica se a tabela existe\n",
    "    spark.sql('use catalog poc_datum')\n",
    "    tabelas_existentes = spark.sql(\"SHOW TABLES\")\n",
    "    if table_name not in [row.tableName for row in tabelas_existentes.collect()]:\n",
    "        # Carrega o DataFrame a partir do arquivo parquet\n",
    "        df = spark.read.parquet(file_path_parquet)\n",
    "\n",
    "        # Salva o Dataframe como uma tabela no Databricks\n",
    "        df.write.format('delta').mode(\"overwrite\").saveAsTable(f'{table_name}')\n",
    "\n",
    "        print(f\"A tabela '{table_name}' foi criada com sucesso no Databricks.\")\n",
    "    else:\n",
    "        print(f\"A tabela '{table_name}' já existe no Databricks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dea22a0-6c94-4039-a361-963244ffa9b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Premissa: \n",
    "  - Carregue o conjunto de dados no Databricks.\n",
    "  - Explore o esquema dos dados e faça ajustes conforme necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc160dd-c396-4c0b-bec2-a334227d1d68",
     "showTitle": true,
     "title": "Lê arquivo .csv e adiciona coluna de controle row_ingestion_timestamp"
    }
   },
   "outputs": [],
   "source": [
    "# Parâmetros de leitura de arquivo\n",
    "path_file_location = dbutils.widgets.get('p_path_file_location') #\"abfss://datum-metastore@proofconceptdatum.dfs.core.windows.net\"\n",
    "layer = dbutils.widgets.get('p_layer') #\"raw/olistbr-brazilian-ecommerce\"\n",
    "file_name_raw = dbutils.widgets.get('p_file_name_raw') #\"olist_customers_dataset\"\n",
    "file_type_raw = dbutils.widgets.get('p_file_type_raw') #\"csv\"\n",
    "\n",
    "# Lê o CSV\n",
    "df = ler_csv(path=path_file_location, layer=layer, file_name_raw=file_name_raw, file_type_raw=file_type_raw)\n",
    "\n",
    "# Adiciona coluna row_ingestion_timestamp\n",
    "df_rits = add_timestamp(df=df)\n",
    "\n",
    "display(df_rits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14110ca-1823-411d-894c-26e7288026af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Premissa: Grave os dados transformados e agregados em um formato Parquet para persistência eficiente.\n",
    "\n",
    "Obs.: Aqui não estamos considerando opções avançadas de carga de dados, como carga incremental, upsert ou Auto Loader, pois este é um cenário de prova de conceito (POC). Entretanto, é crucial considerar essas estratégias em um ambiente de produção para garantir eficiência e precisão na gestão de dados. Além disso, o particionamento de dados também deve ser cuidadosamente avaliado e implementado para otimizar o desempenho e a escalabilidade do sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb1d9d1-e1eb-4207-92de-a4d194dfeef5",
     "showTitle": true,
     "title": "Grava os dados em formato parquet na camada Raw do ASDL"
    }
   },
   "outputs": [],
   "source": [
    "# Parâmetros de gravação de arquivo\n",
    "file_name_bronze = file_name_raw\n",
    "destination_path = f'{path_file_location}/bronze/olistbr-brazilian-ecommerce/{file_name_bronze}'\n",
    "mode_write_bronze = dbutils.widgets.get('p_mode_write_bronze') #'overwrite'\n",
    "\n",
    "# Salva arquivo no ADLS (Azure Data Lake Storage) em formato parquet, na camada bronze\n",
    "gravar_parquet(df_rits, destination_path=destination_path, mode_write_bronze=mode_write_bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2194175968404450,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "RawToBronze",
   "widgets": {
    "p_file_name_raw": {
     "currentValue": "olist_customers_dataset",
     "nuid": "bf6a9e5d-210e-415d-a8cb-3113f338a958",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_file_name_raw",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_file_type_raw": {
     "currentValue": "csv",
     "nuid": "2108ca87-dcff-4729-a182-a13e424bee50",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_file_type_raw",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_layer": {
     "currentValue": "raw/olistbr-brazilian-ecommerce",
     "nuid": "c99068dd-595a-476f-82bb-3c1f590912c7",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_layer",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_mode_write_bronze": {
     "currentValue": "overwrite",
     "nuid": "7cbc73f1-d047-445d-9eff-ce6f8c26a154",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_mode_write_bronze",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_path_file_location": {
     "currentValue": "dbfs:/FileStore/datum/",
     "nuid": "0a4ec350-e818-4d23-b870-61c754aa95f5",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_path_file_location",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
